{
 "cells": [
  {
   "cell_type": "raw",
   "id": "ecb309f7",
   "metadata": {},
   "source": [
    "---\n",
    "title: 'Predicting Belgian Real Estate Prices: Part 4: Building a Baseline Model'\n",
    "author: Adam Cseresznye\n",
    "date: '2023-09-17'\n",
    "categories:\n",
    "  - Predicting Belgian Real Estate Prices \n",
    "jupyter: python3\n",
    "toc: true\n",
    "format:\n",
    "  html:\n",
    "    code-fold: true\n",
    "    code-tools: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab5a692-e95d-4785-8d85-ed1cc10b4056",
   "metadata": {
    "tags": []
   },
   "source": [
    "![Photo by Stephen Phillips - Hostreviews.co.uk on UnSplash](https://cf.bstatic.com/xdata/images/hotel/max1024x768/408003083.jpg?k=c49b5c4a2346b3ab002b9d1b22dbfb596cee523b53abef2550d0c92d0faf2d8b&o=&hp=1){fig-align=\"center\" width=50%}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af189aa",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc3c6a54-530c-4491-bb08-3dfc054d78a6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div id=\"0lHnio\"></div>\n",
       "            <script type=\"text/javascript\" data-lets-plot-script=\"library\">\n",
       "                if(!window.letsPlotCallQueue) {\n",
       "                    window.letsPlotCallQueue = [];\n",
       "                }; \n",
       "                window.letsPlotCall = function(f) {\n",
       "                    window.letsPlotCallQueue.push(f);\n",
       "                };\n",
       "                (function() {\n",
       "                    var script = document.createElement(\"script\");\n",
       "                    script.type = \"text/javascript\";\n",
       "                    script.src = \"https://cdn.jsdelivr.net/gh/JetBrains/lets-plot@v4.0.1/js-package/distr/lets-plot.min.js\";\n",
       "                    script.onload = function() {\n",
       "                        window.letsPlotCall = function(f) {f();};\n",
       "                        window.letsPlotCallQueue.forEach(function(f) {f();});\n",
       "                        window.letsPlotCallQueue = [];\n",
       "                        \n",
       "                    };\n",
       "                    script.onerror = function(event) {\n",
       "                        window.letsPlotCall = function(f) {};    // noop\n",
       "                        window.letsPlotCallQueue = [];\n",
       "                        var div = document.createElement(\"div\");\n",
       "                        div.style.color = 'darkred';\n",
       "                        div.textContent = 'Error loading Lets-Plot JS';\n",
       "                        document.getElementById(\"0lHnio\").appendChild(div);\n",
       "                    };\n",
       "                    var e = document.getElementById(\"0lHnio\");\n",
       "                    e.appendChild(script);\n",
       "                })()\n",
       "            </script>\n",
       "            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import time\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import catboost\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost\n",
    "from data import utils\n",
    "from IPython.display import clear_output, display\n",
    "from lets_plot import *\n",
    "from lets_plot.mapping import as_discrete\n",
    "from sklearn import (\n",
    "    compose,\n",
    "    dummy,\n",
    "    ensemble,\n",
    "    impute,\n",
    "    linear_model,\n",
    "    model_selection,\n",
    "    pipeline,\n",
    "    preprocessing,\n",
    "    svm,\n",
    "    tree,\n",
    ")\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from tqdm import tqdm\n",
    "\n",
    "LetsPlot.setup_html()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8194fbe9-cd5c-4724-b219-b17d11850955",
   "metadata": {},
   "source": [
    "**Objective**:\n",
    "* Examine the necessary sample pre-processing steps before modeling\n",
    "* Create the required pipeline\n",
    "* \n",
    "Evaluate multiple algoritms\n",
    "* \r\n",
    "Choose a suitable baseli modell.\r\n",
    "\r\n",
    "\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86846c91-1d46-4c2e-8fb2-72f5c5b8e8fd",
   "metadata": {},
   "source": [
    "# Prepare dataframe before modelling\n",
    "## Read in the processed file\n",
    "\n",
    "After importing our preprocessed dataframe, a crucial step in our data refinement process involves the culling of certain columns. Specifically, we intend to exclude columns with labels such as \"external_reference,\" \"ad_url,\" \"day_of_retrieval,\" \"website,\" \"reference_number_of_the_epc_report,\" and \"housenumber.\" Our rationale behind this action is to enhance the efficiency of our model by eliminating potentially non-contributory features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f38a1142-1fd1-4e4f-9ca4-08132b07fd0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of dataframe after read-in a pre-processing: (3660, 50)\n",
      "Shape of X: (3660, 49)\n",
      "Shape of y: (3660,)\n"
     ]
    }
   ],
   "source": [
    "utils.seed_everything(utils.Configuration.seed)\n",
    "\n",
    "df = (\n",
    "    pd.read_parquet(\n",
    "        utils.Configuration.INTERIM_DATA_PATH.joinpath(\n",
    "            \"2023-10-01_Processed_dataset_for_NB_use.parquet.gzip\"\n",
    "        )\n",
    "    )\n",
    "    .sample(frac=1, random_state=utils.Configuration.seed)\n",
    "    .reset_index(drop=True)\n",
    "    .assign(price=lambda df: np.log(df.price))\n",
    "    .drop(\n",
    "        columns=[\n",
    "            \"external_reference\",\n",
    "            \"ad_url\",\n",
    "            \"day_of_retrieval\",\n",
    "            \"website\",\n",
    "            \"reference_number_of_the_epc_report\",\n",
    "            \"housenumber\",\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "\n",
    "print(f\"Shape of dataframe after read-in a pre-processing: {df.shape}\")\n",
    "X = df.drop(columns=utils.Configuration.target_col)\n",
    "y = df[utils.Configuration.target_col]\n",
    "\n",
    "print(f\"Shape of X: {X.shape}\")\n",
    "print(f\"Shape of y: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6ea9ae-4cfe-4383-b0ec-d150d0c345e3",
   "metadata": {},
   "source": [
    "## Train-test split\n",
    "\n",
    "The subsequent phase in our data preparation involves the partitioning of our dataset into training and testing subsets. To accomplish this, we'll leverage the `model_selection.train_test_split` method. This step ensures that we have distinct sets for model training and evaluation, a fundamental practice in machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8ca1977-bdfe-41c0-af59-8f97fe7c9be0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X-train: (2928, 49)\n",
      "Shape of X-test: (732, 49)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(\n",
    "    X, y, test_size=0.2, random_state=utils.Configuration.seed\n",
    ")\n",
    "\n",
    "print(f\"Shape of X-train: {X_train.shape}\")\n",
    "print(f\"Shape of X-test: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1eed3a-9abe-415e-b862-b93a0b0464c7",
   "metadata": {},
   "source": [
    "# Implementing the data-processing pipeline\n",
    "\n",
    "In order to compare various machine learning algorithms effectively, our initial approach will involve constructing a straightforward pipeline. This pipeline's primary objective is to segregate columns based on their data types, recognizing the need for distinct preprocessing steps for continuous (numerical) and categorical variables. To facilitate this process within our scikit-learn pipeline, we will begin by implementing a custom class named \"FeatureSelector.\"\r\n",
    "\r\n",
    "The rationale behind thor is to establish a structured approach to feature handlils. The FeatureSelector class will provide us with a streamlined means to access and process columns based on their data typess."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bce834b8-af04-492c-8798-d4498f701191",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureSelector(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    A transformer for selecting specific columns from a DataFrame.\n",
    "\n",
    "    This class inherits from the BaseEstimator and TransformerMixin classes from sklearn.base.\n",
    "    It overrides the fit and transform methods from the parent classes.\n",
    "\n",
    "    Attributes:\n",
    "        feature_names_in_ (list): The names of the features to select.\n",
    "        n_features_in_ (int): The number of features to select.\n",
    "\n",
    "    Methods:\n",
    "        fit(X, y=None): Fit the transformer. Returns self.\n",
    "        transform(X, y=None): Apply the transformation. Returns a DataFrame with selected features.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, feature_names_in_):\n",
    "        \"\"\"\n",
    "        Constructs all the necessary attributes for the FeatureSelector object.\n",
    "\n",
    "        Args:\n",
    "            feature_names_in_ (list): The names of the features to select.\n",
    "        \"\"\"\n",
    "        self.feature_names_in_ = feature_names_in_\n",
    "        self.n_features_in_ = len(feature_names_in_)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Fit the transformer. This method doesn't do anything as no fitting is necessary.\n",
    "\n",
    "        Args:\n",
    "            X (DataFrame): The input data.\n",
    "            y (array-like, optional): The target variable. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            self: The instance itself.\n",
    "        \"\"\"\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Apply the transformation. Selects the features from the input data.\n",
    "\n",
    "        Args:\n",
    "            X (DataFrame): The input data.\n",
    "            y (array-like, optional): The target variable. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            DataFrame: A DataFrame with only the selected features.\n",
    "        \"\"\"\n",
    "        return X.loc[:, self.feature_names_in_].copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7eecbee6-000e-4806-92fd-3bb6826690d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting columns by dtypes\n",
    "\n",
    "numerical_columns = X_train.head().select_dtypes(\"number\").columns.to_list()\n",
    "categorical_columns = X_train.head().select_dtypes(\"object\").columns.to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f158c569-d3d5-4d2d-8c07-2e48324f45fc",
   "metadata": {},
   "source": [
    "Addressing missing values is a crucial preliminary step in our machine learning pipeline, as certain algorithms are sensitive to data gaps. To handle this, we'll employ imputation techniques tailored to the data types of the columns.\n",
    "\n",
    "For numerical columns, we'll adopt the \"median\" strategy for imputation. This approach involves replacing missing values with the median of the available data in the respective numerical column. It's a robust choice for handling missing values in numerical data as it's less sensitive to outliers.\n",
    "\n",
    "Conversely, for categorical columns, we'll opt for imputation using the most frequent values in each column. By filling in missing categorical data with the mode (most common value) for that column, we ensure that the imputed values align with the existing categorical distribution, preserving the integrity of the categorical features.\n",
    "\n",
    "This systematic approach to imputation sets a solid foundation for subsequent machine learning algorithms, ensuring that our dataset is well-prepared for analysis and modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f601bd3f-a3cf-450b-8b95-83c638d7ef29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare pipelines for corresponding columns:\n",
    "numerical_pipeline = pipeline.Pipeline(\n",
    "    steps=[\n",
    "        (\"num_selector\", FeatureSelector(numerical_columns)),\n",
    "        (\"imputer\", impute.SimpleImputer(strategy=\"median\")),\n",
    "        (\"std_scaler\", preprocessing.MinMaxScaler()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "categorical_pipeline = pipeline.Pipeline(\n",
    "    steps=[\n",
    "        (\"cat_selector\", FeatureSelector(categorical_columns)),\n",
    "        (\"imputer\", impute.SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\n",
    "            \"onehot\",\n",
    "            preprocessing.OneHotEncoder(handle_unknown=\"ignore\", sparse_output=True),\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b68e2a-0940-4a11-ace5-0fcce4757149",
   "metadata": {},
   "source": [
    "Once we are satisfied with the individual pipelines designed for numerical and categorical feature processing, the next step involves merging them into a unified pipeline using the `FeatureUnion` method provided by scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25c61e2c-98a3-4ecc-91e4-2a1fb55785a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put all the pipelines inside a FeatureUnion:\n",
    "data_preprocessing_pipeline = pipeline.FeatureUnion(\n",
    "    n_jobs=-1,\n",
    "    transformer_list=[\n",
    "        (\"numerical_pipeline\", numerical_pipeline),\n",
    "        (\"categorical_pipeline\", categorical_pipeline),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf35d5d-31dc-46ce-981d-c579a709d617",
   "metadata": {},
   "source": [
    "# Compare the performance of several algorithms\n",
    "\n",
    "Bringing all these components together in our machine learning pipeline is the culmination of our data preparation and model evaluation process. \r\n",
    "\r\n",
    "1. **Algorithm Selection**We c Choose a set of machine learning algorithms thaweou want to evaluatek.\r\n",
    "\r\n",
    "2. **Data SplittingWe u*: Utilize the `ShuffleSplit` method to generate randomized indices for splittouryour data into training and test sets. This ensures randomness in data selection and is crucial for unbiased evaluation.\r\n",
    "\r\n",
    "3. **Model Training and Evaluation**: For each selected algorwe followfollow these steps:\r\n",
    "   - Fit the model on the training data.\r\n",
    "   - Evaluate the model using negative mean squared error (`neg_mean_squared_error`) as the scoring metric.\r\n",
    "   - Record the training and test scores, as well as the standard deviation of scores to assess model s\n",
    "   - Measure the time taken to fit each model, which provides insights into computational efficiency.el performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dfd649bd-cc18-4f89-bd45-2e11d4fe0ddf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MLA Name</th>\n",
       "      <th>MLA Parameters</th>\n",
       "      <th>MLA Train RMSE Mean</th>\n",
       "      <th>MLA Test RMSE Mean</th>\n",
       "      <th>MLA Test RMSE 3*STD</th>\n",
       "      <th>MLA Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>CatBoostRegressor</td>\n",
       "      <td>{'loss_function': 'RMSE', 'silent': True}</td>\n",
       "      <td>-0.016777</td>\n",
       "      <td>-0.069782</td>\n",
       "      <td>0.0099</td>\n",
       "      <td>3.244442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LGBMRegressor</td>\n",
       "      <td>{'boosting_type': 'gbdt', 'class_weight': None...</td>\n",
       "      <td>-0.012073</td>\n",
       "      <td>-0.077218</td>\n",
       "      <td>0.006721</td>\n",
       "      <td>0.1577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ExtraTreesRegressor</td>\n",
       "      <td>{'bootstrap': False, 'ccp_alpha': 0.0, 'criter...</td>\n",
       "      <td>-0.000005</td>\n",
       "      <td>-0.085609</td>\n",
       "      <td>0.011404</td>\n",
       "      <td>21.911669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>GradientBoostingRegressor</td>\n",
       "      <td>{'alpha': 0.9, 'ccp_alpha': 0.0, 'criterion': ...</td>\n",
       "      <td>-0.050419</td>\n",
       "      <td>-0.085817</td>\n",
       "      <td>0.012371</td>\n",
       "      <td>1.716229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>XGBRegressor</td>\n",
       "      <td>{'objective': 'reg:squarederror', 'base_score'...</td>\n",
       "      <td>-0.005383</td>\n",
       "      <td>-0.087365</td>\n",
       "      <td>0.013097</td>\n",
       "      <td>0.207547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>RandomForestRegressor</td>\n",
       "      <td>{'bootstrap': True, 'ccp_alpha': 0.0, 'criteri...</td>\n",
       "      <td>-0.012561</td>\n",
       "      <td>-0.089146</td>\n",
       "      <td>0.012937</td>\n",
       "      <td>18.576334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>AdaBoostRegressor</td>\n",
       "      <td>{'base_estimator': 'deprecated', 'estimator': ...</td>\n",
       "      <td>-0.104764</td>\n",
       "      <td>-0.12835</td>\n",
       "      <td>0.014058</td>\n",
       "      <td>0.738398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SVR</td>\n",
       "      <td>{'C': 1.0, 'cache_size': 200, 'coef0': 0.0, 'd...</td>\n",
       "      <td>-0.051323</td>\n",
       "      <td>-0.129455</td>\n",
       "      <td>0.023128</td>\n",
       "      <td>0.393076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>DecisionTreeRegressor</td>\n",
       "      <td>{'ccp_alpha': 0.0, 'criterion': 'squared_error...</td>\n",
       "      <td>-0.000005</td>\n",
       "      <td>-0.1877</td>\n",
       "      <td>0.035284</td>\n",
       "      <td>0.325504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PassiveAggressiveRegressor</td>\n",
       "      <td>{'C': 1.0, 'average': False, 'early_stopping':...</td>\n",
       "      <td>-0.110837</td>\n",
       "      <td>-0.380512</td>\n",
       "      <td>0.822766</td>\n",
       "      <td>0.044286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SGDRegressor</td>\n",
       "      <td>{'alpha': 0.0001, 'average': False, 'early_sto...</td>\n",
       "      <td>-0.203434</td>\n",
       "      <td>-0.414895</td>\n",
       "      <td>0.516956</td>\n",
       "      <td>0.063055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Lasso</td>\n",
       "      <td>{'alpha': 1.0, 'copy_X': True, 'fit_intercept'...</td>\n",
       "      <td>-0.561411</td>\n",
       "      <td>-0.558661</td>\n",
       "      <td>0.043811</td>\n",
       "      <td>0.036866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>DummyClassifier</td>\n",
       "      <td>{'constant': None, 'random_state': None, 'stra...</td>\n",
       "      <td>-0.770908</td>\n",
       "      <td>-0.778205</td>\n",
       "      <td>0.965141</td>\n",
       "      <td>0.03528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RANSACRegressor</td>\n",
       "      <td>{'estimator': None, 'is_data_valid': None, 'is...</td>\n",
       "      <td>-0.74997</td>\n",
       "      <td>-1.14233</td>\n",
       "      <td>2.855596</td>\n",
       "      <td>16.374684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LinearRegression</td>\n",
       "      <td>{'copy_X': True, 'fit_intercept': True, 'n_job...</td>\n",
       "      <td>-0.063432</td>\n",
       "      <td>-2.076803</td>\n",
       "      <td>5.451463</td>\n",
       "      <td>1.8297</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      MLA Name  \\\n",
       "11           CatBoostRegressor   \n",
       "12               LGBMRegressor   \n",
       "9          ExtraTreesRegressor   \n",
       "6    GradientBoostingRegressor   \n",
       "13                XGBRegressor   \n",
       "8        RandomForestRegressor   \n",
       "10           AdaBoostRegressor   \n",
       "5                          SVR   \n",
       "7        DecisionTreeRegressor   \n",
       "2   PassiveAggressiveRegressor   \n",
       "1                 SGDRegressor   \n",
       "4                        Lasso   \n",
       "14             DummyClassifier   \n",
       "3              RANSACRegressor   \n",
       "0             LinearRegression   \n",
       "\n",
       "                                       MLA Parameters MLA Train RMSE Mean  \\\n",
       "11          {'loss_function': 'RMSE', 'silent': True}           -0.016777   \n",
       "12  {'boosting_type': 'gbdt', 'class_weight': None...           -0.012073   \n",
       "9   {'bootstrap': False, 'ccp_alpha': 0.0, 'criter...           -0.000005   \n",
       "6   {'alpha': 0.9, 'ccp_alpha': 0.0, 'criterion': ...           -0.050419   \n",
       "13  {'objective': 'reg:squarederror', 'base_score'...           -0.005383   \n",
       "8   {'bootstrap': True, 'ccp_alpha': 0.0, 'criteri...           -0.012561   \n",
       "10  {'base_estimator': 'deprecated', 'estimator': ...           -0.104764   \n",
       "5   {'C': 1.0, 'cache_size': 200, 'coef0': 0.0, 'd...           -0.051323   \n",
       "7   {'ccp_alpha': 0.0, 'criterion': 'squared_error...           -0.000005   \n",
       "2   {'C': 1.0, 'average': False, 'early_stopping':...           -0.110837   \n",
       "1   {'alpha': 0.0001, 'average': False, 'early_sto...           -0.203434   \n",
       "4   {'alpha': 1.0, 'copy_X': True, 'fit_intercept'...           -0.561411   \n",
       "14  {'constant': None, 'random_state': None, 'stra...           -0.770908   \n",
       "3   {'estimator': None, 'is_data_valid': None, 'is...            -0.74997   \n",
       "0   {'copy_X': True, 'fit_intercept': True, 'n_job...           -0.063432   \n",
       "\n",
       "   MLA Test RMSE Mean MLA Test RMSE 3*STD   MLA Time  \n",
       "11          -0.069782              0.0099   3.244442  \n",
       "12          -0.077218            0.006721     0.1577  \n",
       "9           -0.085609            0.011404  21.911669  \n",
       "6           -0.085817            0.012371   1.716229  \n",
       "13          -0.087365            0.013097   0.207547  \n",
       "8           -0.089146            0.012937  18.576334  \n",
       "10           -0.12835            0.014058   0.738398  \n",
       "5           -0.129455            0.023128   0.393076  \n",
       "7             -0.1877            0.035284   0.325504  \n",
       "2           -0.380512            0.822766   0.044286  \n",
       "1           -0.414895            0.516956   0.063055  \n",
       "4           -0.558661            0.043811   0.036866  \n",
       "14          -0.778205            0.965141    0.03528  \n",
       "3            -1.14233            2.855596  16.374684  \n",
       "0           -2.076803            5.451463     1.8297  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [11:25<00:00, 45.68s/it]\n"
     ]
    }
   ],
   "source": [
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "\n",
    "    MLA = [\n",
    "        linear_model.LinearRegression(),\n",
    "        linear_model.SGDRegressor(),\n",
    "        linear_model.PassiveAggressiveRegressor(),\n",
    "        linear_model.RANSACRegressor(),\n",
    "        linear_model.Lasso(),\n",
    "        svm.SVR(),\n",
    "        ensemble.GradientBoostingRegressor(),\n",
    "        tree.DecisionTreeRegressor(),\n",
    "        ensemble.RandomForestRegressor(),\n",
    "        ensemble.ExtraTreesRegressor(),\n",
    "        ensemble.AdaBoostRegressor(),\n",
    "        catboost.CatBoostRegressor(silent=True),\n",
    "        lgb.LGBMRegressor(verbose=-1),\n",
    "        xgboost.XGBRegressor(verbosity=0),\n",
    "        dummy.DummyClassifier(),\n",
    "    ]\n",
    "\n",
    "    # note: this is an alternative to train_test_split\n",
    "    cv_split = model_selection.ShuffleSplit(\n",
    "        n_splits=10, test_size=0.3, train_size=0.6, random_state=0\n",
    "    )  # run model 10x with 60/30 split intentionally leaving out 10%\n",
    "\n",
    "    # create table to compare MLA metrics\n",
    "    MLA_columns = [\n",
    "        \"MLA Name\",\n",
    "        \"MLA Parameters\",\n",
    "        \"MLA Train RMSE Mean\",\n",
    "        \"MLA Test RMSE Mean\",\n",
    "        \"MLA Test RMSE 3*STD\",\n",
    "        \"MLA Time\",\n",
    "    ]\n",
    "    MLA_compare = pd.DataFrame(columns=MLA_columns)\n",
    "\n",
    "    # index through MLA and save performance to table\n",
    "    row_index = 0\n",
    "    for alg in tqdm(MLA):\n",
    "        # set name and parameters\n",
    "        MLA_name = alg.__class__.__name__\n",
    "        MLA_compare.loc[row_index, \"MLA Name\"] = MLA_name\n",
    "        MLA_compare.loc[row_index, \"MLA Parameters\"] = str(alg.get_params())\n",
    "\n",
    "        model_pipeline = pipeline.Pipeline(\n",
    "            steps=[\n",
    "                (\"data_preprocessing_pipeline\", data_preprocessing_pipeline),\n",
    "                (\"model\", alg),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        cv_results = model_selection.cross_validate(\n",
    "            model_pipeline,\n",
    "            X_train,\n",
    "            y_train,\n",
    "            cv=cv_split,\n",
    "            scoring=\"neg_mean_squared_error\",\n",
    "            return_train_score=True,\n",
    "        )\n",
    "\n",
    "        MLA_compare.loc[row_index, \"MLA Time\"] = cv_results[\"fit_time\"].mean()\n",
    "        MLA_compare.loc[row_index, \"MLA Train RMSE Mean\"] = cv_results[\n",
    "            \"train_score\"\n",
    "        ].mean()\n",
    "        MLA_compare.loc[row_index, \"MLA Test RMSE Mean\"] = cv_results[\n",
    "            \"test_score\"\n",
    "        ].mean()\n",
    "        MLA_compare.loc[row_index, \"MLA Test RMSE 3*STD\"] = (\n",
    "            cv_results[\"test_score\"].std() * 3\n",
    "        )\n",
    "\n",
    "        row_index += 1\n",
    "\n",
    "        clear_output(wait=True)\n",
    "        display(MLA_compare.sort_values(by=[\"MLA Test RMSE Mean\"], ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31c07d9-52bf-44a7-adeb-237a2d501134",
   "metadata": {},
   "source": [
    "It's evident from the table above that the `CatBoostRegressor` has demonstrated the most promising performance, achieving a score of 0.0697 on the test set. Following closely are the `LGBMRegressor`, `ExtraTreesRegressor`, and `GradientBoostingRegressor`, with `XGBRegressor` trailing behind.\n",
    "\n",
    "In the next section, we'll delve deeper into the optimization of our model. This exploration will include refining model settings, crafting improved features, and employing various techniques to enhance overall predictive accuracy. See you in the next section!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
