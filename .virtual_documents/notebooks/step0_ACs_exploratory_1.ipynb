








import json
import re
import time
from pathlib import Path

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import plotly.express as px
import requests
import seaborn as sns
import tqdm
from bs4 import BeautifulSoup
from geopy.geocoders import Nominatim
from lets_plot import *
from lets_plot.mapping import as_discrete
from requests_html import HTMLSession
from scipy import stats
# selenium 4
from selenium import webdriver
from selenium.webdriver.chrome.service import Service as ChromeService
from webdriver_manager.chrome import ChromeDriverManager

LetsPlot.setup_html()








# this one needs selenium


def find_last_page(soup):
    divs = int(soup.find_all("span", class_="button__label")[-1].text)
    return divs


def extract_selenium(type_, page):
    """
    Extracts and returns the BeautifulSoup object from a specified Immoweb search page.

    Args:
        type_ (str): The type of property to search for (e.g., "rent", "sale").
        page (int): The page number of the search results.

    Returns:
        BeautifulSoup: A BeautifulSoup object containing the parsed HTML content of the Immoweb search page.
    """
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.5060.134 Safari/537.36"
    }
    url = f"https://www.immoweb.be/en/search/house/for-{type_}?countries=BE&page={page}&orderBy=relevance"
    driver = webdriver.Chrome(executable_path=ChromeService(ChromeDriverManager().install()))

    driver.get(url)
    soup = BeautifulSoup(driver.page_source, "lxml")
    return soup





rent_last_page=find_last_page(extract_selenium('rent',1)) + 1


rent_last_page


%%script echo skipping

buy_last_page=find_last_page(extract_selenium('buy',1)) + 1


%%script echo skipping

hitlist = []

for i in tqdm.tqdm(range(1, buy_last_page)):
    results = extract(type_ = 'sale',page = i)
    transform(results)
    time.sleep(2)

(pd.DataFrame(hitlist)
 .to_parquet('for_sale.parquet.gzip')
)


%%script echo skipping
hitlist = []

for i in tqdm.tqdm(range(1, 5)):
    results = extract(type_ = 'rent',page = i)
    transform(results)
    time.sleep(2)

(pd.DataFrame(hitlist)
 .to_parquet('for_rent.parquet.gzip')
)


df_sale = pd.read_parquet("for_sale.parquet.gzip")
df_rent = pd.read_parquet("for_rent.parquet.gzip")








%%script echo skipping

locations = []

unique_apartment_locations = (df_rent['ZIP'].astype(str) + ',' + df_rent['city']).unique()

geolocator = Nominatim(user_agent="myApp")

for idx, element in tqdm.tqdm(enumerate(unique_apartment_locations)):
    try:
        location = geolocator.geocode(unique_apartment_locations[idx])
        case = {
            'latitude' : location.latitude,
            'longitude' : location.longitude,
            'address' : location.address
        }

        locations.append(case)

    except AttributeError:
        location = geolocator.geocode(unique_apartment_locations[idx])
        case = {
            'latitude' : np.nan,
            'longitude' : np.nan,
            'address' : np.nan
        }

        locations.append(case)
pd.DataFrame(locations).to_parquet('locations_for_rent.parquet.gzip', compression='gzip') #saving coordinates to disk


locations = pd.read_parquet(
    "locations_for_rent.parquet.gzip"
)  # reading back the saved locations data
locations


locations2 = pd.concat(
    [locations, pd.Series(unique_apartment_locations, name="unique_address")], axis=1
)


(
    df_rent.assign(
        ZIP_city=lambda df: df["ZIP"].astype(str) + "," + df["city"],
        full_address=lambda df: df.ZIP_city.map(
            locations2.set_index("unique_address").address.to_dict()
        ),
        latitude=lambda df: df.ZIP_city.map(
            locations2.set_index("unique_address").latitude.to_dict()
        ),
        longitude=lambda df: df.ZIP_city.map(
            locations2.set_index("unique_address").longitude.to_dict()
        ),
    )
)
































# | fig-cap: "Most Commonly Used Terms in Spam Messages"
# | label: fig-fig1
